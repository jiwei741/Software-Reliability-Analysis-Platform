<div style="text-align:center;">

<img src="image.png" alt="" style="display:block; margin:0 auto; max-width:60%;">

<h1 style="text-align:center;">软件可靠性课程设计</h1>

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

<div style="width:100%; text-align:center;">
  <table style="margin:0 auto; text-align:center; border-collapse:collapse; display:inline-table;">
    <tr><td style="padding:6px 16px;">题目</td><td style="padding:6px 16px;">软件可靠性分析平台</td></tr>
    <tr><td style="padding:6px 16px;">院系</td><td style="padding:6px 16px;">计算机科学与技术学院</td></tr>
    <tr><td style="padding:6px 16px;">专业</td><td style="padding:6px 16px;">软件工程</td></tr>
    <tr><td style="padding:6px 16px;">学号</td><td style="padding:6px 16px;">202330219</td></tr>
    <tr><td style="padding:6px 16px;">姓名</td><td style="padding:6px 16px;">杨恩祺</td></tr>
    <tr><td style="padding:6px 16px;">指导老师</td><td style="padding:6px 16px;">张德平</td></tr>
  </table>
</div>
<br>
二O二五年12月
</div>

<div style="page-break-after: always;"></div>

# 目录

<div style="font-size:16px; line-height:1.6; margin-top:12px;">
<a style="text-decoration:none; color:inherit;" href="#sec1">1. 公共预处理与入口</a><br/>
&emsp;<a style="text-decoration:none; color:inherit;" href="#sec1-1">1.1 预处理：`_normalize_records`</a><br/>
&emsp;<a style="text-decoration:none; color:inherit;" href="#sec1-2">1.2 统一出口：`build_chart_payload`</a><br/>
<a style="text-decoration:none; color:inherit;" href="#sec2">2. 经典可靠性模型</a><br/>
&emsp;<a style="text-decoration:none; color:inherit;" href="#sec2-1">2.1 Goel-Okumoto（NHPP）</a><br/>
&emsp;<a style="text-decoration:none; color:inherit;" href="#sec2-2">2.2 Jelinski-Moranda</a><br/>
&emsp;<a style="text-decoration:none; color:inherit;" href="#sec2-3">2.3 Musa-Okumoto</a><br/>
&emsp;<a style="text-decoration:none; color:inherit;" href="#sec2-4">2.4 Crow-AMSAA（威布尔增长）</a><br/>
&emsp;<a style="text-decoration:none; color:inherit;" href="#sec2-5">2.5 Duane 模型</a><br/>
<a style="text-decoration:none; color:inherit;" href="#sec3">3. 人工智能模型（BPN / RBF / SVM / GEP）</a><br/>
&emsp;<a style="text-decoration:none; color:inherit;" href="#sec3-1">3.1 数据流与前置分割</a><br/>
&emsp;<a style="text-decoration:none; color:inherit;" href="#sec3-2">3.2 公共特征与工具</a><br/>
&emsp;<a style="text-decoration:none; color:inherit;" href="#sec3-3">3.3 线性回归基线（梯度下降）</a><br/>
&emsp;<a style="text-decoration:none; color:inherit;" href="#sec3-4">3.4 BPN（单隐层前馈网络，Sigmoid）</a><br/>
&emsp;<a style="text-decoration:none; color:inherit;" href="#sec3-5">3.5 RBF（径向基）</a><br/>
&emsp;<a style="text-decoration:none; color:inherit;" href="#sec3-6">3.6 线性 SVM（回归近似）</a><br/>
&emsp;<a style="text-decoration:none; color:inherit;" href="#sec3-7">3.7 GEP（二阶符号回归）</a><br/>
&emsp;<a style="text-decoration:none; color:inherit;" href="#sec3-8">3.8 训练参数与实时性权衡</a><br/>
&emsp;<a style="text-decoration:none; color:inherit;" href="#sec3-9">3.9 指标与图表对应关系</a><br/>
&emsp;<a style="text-decoration:none; color:inherit;" href="#sec3-10">3.10 简单实验分析</a><br/>
<a style="text-decoration:none; color:inherit;" href="#sec4">4. 时间序列模型</a><br/>
&emsp;<a style="text-decoration:none; color:inherit;" href="#sec4-1">4.1 ARIMA(1,1,1) 近似</a><br/>
&emsp;<a style="text-decoration:none; color:inherit;" href="#sec4-2">4.2 Holt-Winters（三指数平滑）</a><br/>
<a style="text-decoration:none; color:inherit;" href="#sec5">5. 组合加权模型</a><br/>
<a style="text-decoration:none; color:inherit;" href="#sec6">6. SDA 场景模型</a><br/>
<a style="text-decoration:none; color:inherit;" href="#sec7">7. 建模与验证流程总结</a><br/>
<a style="text-decoration:none; color:inherit;" href="#sec8">8. 使用与检查</a><br/>
<a style="text-decoration:none; color:inherit;" href="#sec9">9. 可深化方向</a><br/>
<a style="text-decoration:none; color:inherit;" href="#sec10">10. 模型合理性与风险提示</a><br/>
<a style="text-decoration:none; color:inherit;" href="#sec11">11. 真实数据集对比实验</a><br/>
&emsp;<a style="text-decoration:none; color:inherit;" href="#sec11-1">11.1 优化后再评测（特征缩放 + 早停）</a><br/>
<a style="text-decoration:none; color:inherit;" href="#sec12">12. 模型算法总结</a><br/>
</div>

<div style="page-break-after: always;"></div>

# 个人算法报告

本文面向平台已落地的可靠性模型，全面解析建模过程、核心函数与变量含义，并给出对应代码片段（均来自 `services/reliability.py`）。数据在进入各模型前，统一经 `_normalize_records` 规范化为 `module` / `failures` / `mtbf` / `runtime` / `timestamp` / `source`，后续函数均基于此一致结构。

<a id="sec1"></a>

## 1. 公共预处理与入口

<a id="sec1-1"></a>

### 1.1 预处理：`_normalize_records`
- 作用：清洗每条记录，将缺省/异常值替换为可用数值，并补齐时间戳与来源。
- 关键变量：  
  - `failures`：失效次数，强制为非负整数。  
  - `mtbf`：平均无故障时间，默认 1.0。  
  - `runtime`：运行时长，若缺省则等于 `mtbf`。  
  - `timestamp`：缺省则补当前 UTC 时间。  
  - `source`：数据来源标记。

<a id="sec1-2"></a>

### 1.2 统一出口：`build_chart_payload`
- 作用：把预处理后的记录生成 16 组图表数据（经典 5、AI 4、时间序列 2、组合 2、SDA 1、对比 2）。
- 核心流程：  
  1) 归一化并提取 `failures` / `runtime` / `mtbf` / `times`。  
  2) 训练简化 AI 模型（线性回归）并拿到预测、RMSE。  
  3) 调用各经典/时间序列/组合/SDA 函数生成序列。  
  4) 按图表定义封装成前端可用的 datasets。

<a id="sec2"></a>

## 2. 经典可靠性模型

<a id="sec2-1"></a>

### 2.1 Goel-Okumoto（NHPP）
- 模型公式：$$ m(t) = a \left(1 - e^{-bt}\right) $$  
其中 \(a\) 为预期总失效数，\(b\) 为强度衰减系数。
- 代码与变量：
```python
def _goel_okumoto_series(a: float, b: float, times: Sequence[int]) -> Dict[str, List[float]]:
    cumulative, intensity, mtbf = [], [], []
    for t in times:
        n_t = a * (1 - math.exp(-b * t))      # 累计失效数
        lam_t = a * b * math.exp(-b * t)      # 瞬时失效率
        cumulative.append(round(n_t, 2))
        intensity.append(round(lam_t, 3))
        mtbf.append(round(1 / max(lam_t, 1e-3), 2))  # 对应 MTBF = 1/λ
    return {"cumulative": cumulative, "intensity": intensity, "mtbf": mtbf}
```
- 说明：对时间序列 `times` 逐点计算累计失效与强度，防止 λ 过小导致除零。
- 变量补充：`times` 在 `build_chart_payload` 中用样本长度生成 `[1..n]`；当没有时间戳时用序号代替；`a`、`b` 取决于总失效、总时长，当前实现用 `a=total_failures*1.1`、`b=1/total_runtime` 作为启发式估计。

**实验示例（Goel-Okumoto）**  
- 场景：运行 500 小时，累计 14 次失效，阶段=5。  
- 参数：`a=15.4, b=1/500=0.002`。  
- 过程：生成 `times=[1..5]`，计算累计失效与强度。  
- 结果：累计失效曲线平滑递增，强度随时间指数衰减；MTBF 随时间上升。  
- 风险：若实际失效集中在早期，指数衰减可能低估后期强度，可调大 `b` 或缩短阶段长度。

<a id="sec2-2"></a>

### 2.2 Jelinski-Moranda
- 公式：$$ \lambda_i = \phi (N - i + 1) $$，剩余缺陷越少，失效率越低。
```python
def _jelinski_moranda_series(total_defects: int, phi: float, intervals: Sequence[int]) -> Dict[str, List[float]]:
    intensity, reliability = [], []
    remaining = total_defects
    for delta_t in intervals:
        lam = phi * remaining                 # 当前失效率
        intensity.append(round(lam, 3))
        reliability.append(round(math.exp(-lam * max(delta_t, 1)), 3))  # 区间可靠度 e^{-λΔt}
        remaining = max(remaining - 1, 1)     # 假设每阶段修复 1 个缺陷
    return {"intensity": intensity, "reliability": reliability}
```
- 说明：`delta_t` 为相邻失效间隔，`remaining` 逐步递减以体现修复效果。
- 变量补充：`total_defects` 在 `build_chart_payload` 中取 `max(total_failures*1.2, len(times)+1)`，避免缺陷数过小；`intervals` 取运行时长或 1，防止 0 间隔导致指数溢出。

**实验示例（JM）**  
- 场景：5 个阶段，初始缺陷估计 18，`phi=1/500=0.002`，每阶段耗时约 100。  
- 过程：每阶段强度递减，可靠度用 `exp(-λΔt)` 计算。  
- 结果：强度由 ~0.036 逐步下降，可靠度稳步提升。  
- 风险：假设每阶段修 1 个缺陷，若真实修复量更大/更小需调整 `remaining` 更新策略。

<a id="sec2-3"></a>

### 2.3 Musa-Okumoto
- 公式：$$ m(t) = \frac{1}{\theta}\ln(1+\theta\lambda_0 t) $$
```python
def _musa_okumoto_series(theta: float, phi: float, times: Sequence[float]) -> Dict[str, List[float]]:
    cumulative, mean_intensity = [], []
    for t in times:
        m_t = theta * math.log(1 + phi * t)   # 累计失效期望
        lam_t = (theta * phi) / (1 + phi * t) # 平均失效率
        cumulative.append(round(m_t, 2))
        mean_intensity.append(round(lam_t, 3))
    return {"cumulative": cumulative, "intensity": mean_intensity}
```
- 说明：`theta` 近似总故障因子，`phi` 为单位时间强度，随时间衰减。
- 变量补充：`theta` 由 `total_failures/log(1+avg_runtime)` 得到，`phi=1/avg_runtime`；两者均做下限保护，避免 0 或负值。

**实验示例（Musa-Okumoto）**  
- 场景：平均运行时长 480h，总失效 14。  
- 参数：`theta≈14/ln(1+480)≈2.33`，`phi≈1/480≈0.00208`。  
- 过程：按运行时长序列计算累计失效与强度。  
- 结果：累计失效呈对数增长，平均强度随时间缓降。  
- 风险：当运行时长差异极大时，可能导致前期/后期偏差，需要分段建模或重估 `theta`。

<a id="sec2-4"></a>

### 2.4 Crow-AMSAA（威布尔增长）
- 公式：$$ N(t) = \lambda t^\beta $$，\(\beta>1\) 表示改进，\(\beta<1\) 表示退化。
```python
def _crow_amsaa_series(lam: float, beta: float, phases: Sequence[int]) -> Dict[str, List[float]]:
    cumulative, growth_rate = [], []
    for t in phases:
        n_t = lam * (t ** beta)               # 累计失效
        r_t = lam * beta * (t ** (beta - 1))  # 增长率
        cumulative.append(round(n_t, 2))
        growth_rate.append(round(r_t, 3))
    return {"cumulative": cumulative, "growth": growth_rate}
```
- 说明：`lam`/`beta` 由总失效与阶段推估，增长率曲线用于评估改进趋势。
- 变量补充：`lam` 下限为 0.5，避免初期无故障导致全零；`beta` 依据总失效和样本数拉伸，反映改进速度。

**实验示例（Crow-AMSAA）**  
- 场景：阶段数 4，首阶段失效 2，总失效 14。  
- 参数：`lambda=2`（下限保护为 0.5，但此处大于 0.5），`beta≈1+14/(4*50)=1.07`。  
- 结果：累计失效近似线性偏上升，增长率略递增，表明轻度改进。  
- 风险：若首批失效极小，曲线趋平，可通过调整 `beta` 或合并阶段改善拟合。

<a id="sec2-5"></a>

### 2.5 Duane 模型
- 公式：$$ \text{MTBF}(t) = K \, t^{1-\alpha} $$
```python
def _duane_growth_series(alpha: float, beta: float, phases: Sequence[int]) -> Dict[str, List[float]]:
    return {"slopes": [round(alpha * (t ** (-beta)), 3) for t in phases]}
```
- 说明：用幂函数体现随时间的 MTBF 改善，`beta` 控制斜率衰减。
- 变量补充：`alpha` 取 `total_failures/total_runtime`，`beta` 固定 0.25 作为中等改进速率；若需更精确可基于线性回归估计幂指数。

**实验示例（Duane）**  
- 场景：总失效 14，总时长 500。  
- 参数：`alpha=14/500=0.028`，`beta=0.25`。  
- 结果：斜率序列随阶段轻微下降，体现改进；若希望更快改善，可提高 `beta`。  
- 风险：若数据呈下降 MTBF，幂模型可能高估，需调整 `beta` 或改用其他增长模型。

> 以上经典模型在 `build_chart_payload` 中，使用总失效、平均 MTBF、总时长等自适应参数驱动 5 张经典图表。

<a id="sec3"></a>

## 3. 人工智能模型（BPN / RBF / SVM / GEP）
为保证每个模型有真实训练过程，AI 区域实现了一组轻量训练器（全批量梯度下降），确保秒级完成。

<a id="sec3-1"></a>

### 3.1 数据流与前置分割
- 数据进入 AI 区域前，先经 `_normalize_records` 确保 `failures`、`runtime`、`mtbf` 都为浮点数且非负。
- `_prepare_features` 将记录转换为特征矩阵 `X=[[failures, runtime], ...]` 与标签 `y=[mtbf, ...]`。
- 在 `build_chart_payload` 中不再只用线性预测，而是同时训练 BPN/RBF/SVM/GEP 四套模型；当样本 <3 时回退到原始 `mtbf` 以避免过拟合或训练崩溃。

<a id="sec3-2"></a>

### 3.2 公共特征与工具
```python
def _prepare_features(records):
    X = [[float(r["failures"]), float(r["runtime"])] for r in records]
    y = [float(r["mtbf"]) for r in records]
    return X, y

def _rmse(y_true, y_pred):
    mse = sum((a - b) ** 2 for a, b in zip(y_true, y_pred)) / max(len(y_true) or 1, 1)
    return round(math.sqrt(mse), 4)
```

<a id="sec3-3"></a>

### 3.3 线性回归基线（梯度下降）
```python
def _train_linear_gd(X, y, lr=1e-4, epochs=200):
    w = [0.0, 0.0, 0.0]  # [bias, w1, w2]
    n = max(len(X), 1)
    for _ in range(epochs):
        grad = [0.0, 0.0, 0.0]
        for xi, yi in zip(X, y):
            pred = w[0] + w[1]*xi[0] + w[2]*xi[1]
            error = pred - yi
            grad[0] += error
            grad[1] += error * xi[0]
            grad[2] += error * xi[1]
        for i in range(3):
            w[i] -= lr * grad[i] / n
    preds = [w[0] + w[1]*xi[0] + w[2]*xi[1] for xi in X]
    return {"coeffs": w, "pred": preds}
```
- 说明：全批量梯度下降，得到系数与预测，RMSE 作为基线。

<a id="sec3-4"></a>

### 3.4 BPN（单隐层前馈网络，Sigmoid）
```python
def _train_bpn(X, y, hidden=6, lr=3e-3, epochs=180):
    # 初始化 W1(hidden×2), b1, W2(hidden), b2
    ...
    def forward(xi):
        h_raw = [sum(w*x for w,x in zip(W1[i], xi)) + b1[i] for i in range(hidden)]
        h = [sigmoid(v) for v in h_raw]
        out = sum(w*h_i for w,h_i in zip(W2, h)) + b2
        return h, out
    for _ in range(epochs):
        # 累积梯度，全批量更新 W1/W2/b1/b2
        ...
    preds = [forward(xi)[1] for xi in X]
    return {"pred": preds, "weights": {"W1": W1, "W2": W2, "b1": b1, "b2": b2}}
```
- 说明：Sigmoid 激活 + MSE，训练 180 轮，`chart-bpn` 使用实际 vs BPN 预测。

<a id="sec3-5"></a>

### 3.5 RBF（径向基）
```python
def _train_rbf(X, y, centers=3, sigma=1.0, lr=4e-3, epochs=150):
    chosen = 随机选中心
    weights = [0.0]*centers
    def phi(xi, cj):
        dist2 = sum((a-b)**2 for a,b in zip(xi,cj))
        return exp(-dist2/(2*sigma*sigma))
    for _ in range(epochs):
        grad = [0.0]*centers
        for xi, yi in zip(X,y):
            pred = sum(w*phi(xi,c) for w,c in zip(weights, chosen))
            error = pred - yi
            for j in range(centers):
                grad[j] += error * phi(xi, chosen[j])
        weights[j] -= lr * grad[j] / n
    preds = [sum(w*phi(x,c) for w,c in zip(weights, chosen)) for x in X]
    return {"pred": preds, "weights": weights, "centers": chosen}
```
- 说明：高斯基函数 + 梯度下降求权重，`chart-rbf` 显示误差与形态。

<a id="sec3-6"></a>

### 3.6 线性 SVM（回归近似）
```python
def _train_svm_linear(X, y, lr=2e-4, epochs=200, C=0.5):
    w = [0.0,0.0,0.0]; eps = 1.0
    for _ in range(epochs):
        grad = [0.0,0.0,0.0]
        for xi, yi in zip(X,y):
            pred = w[0] + w[1]*xi[0] + w[2]*xi[1]
            margin = abs(pred-yi)
            if margin > eps:
                sign = 1 if pred > yi else -1
                grad[0] += sign
                grad[1] += sign * xi[0]
                grad[2] += sign * xi[1]
        w = [w[i] * (1 - lr * C) - lr * grad[i] / max(len(X),1) for i in range(3)]
    preds = [w[0] + w[1]*xi[0] + w[2]*xi[1] for xi in X]
    return {"weights": w, "pred": preds}
```
- 说明：近似 hinge 损失 + L2 正则，输出权重柱状用于 `chart-svm`。

<a id="sec3-7"></a>

### 3.7 GEP（符号回归 -> 二阶多项式回归）
```python
def _train_gep_poly(X, y, lr=2e-5, epochs=200):
    weights = [0.0]*6  # [1, f, r, f^2, r^2, f*r]
    def features(xi): f,r = xi; return [1.0, f, r, f*f, r*r, f*r]
    for _ in range(epochs):
        grad = [0.0]*6
        for xi, yi in zip(X,y):
            feats = features(xi)
            pred = sum(w*v for w,v in zip(weights, feats))
            error = pred - yi
            for i in range(6): grad[i] += error * feats[i]
        weights = [w - lr * g / max(len(X),1) for w,g in zip(weights, grad)]
    preds = [sum(w*v for w,v in zip(weights, features(xi))) for xi in X]
    return {"weights": weights, "pred": preds}
```
- 说明：用二阶多项式拟合 MTBF，提供可解释的“符号”形式；`chart-gep` 展示预测 vs runtime。

> 这些训练器在 `build_chart_payload` 中按需调用，BPN/RBF/SVM/GEP 预测直接驱动对应图表，避免“只套公式不训练”。

<a id="sec3-8"></a>

### 3.8 训练参数与实时性权衡
- 学习率与轮数选择：BPN/RBF/SVM/GEP 轮数 150~200、学习率 1e-5~4e-3，保证 10~50 条样本时训练耗时 < 0.1s。
- 退化保护：样本不足或梯度病态时，返回零权重或原始 MTBF，前端仍可正常渲染。
- 精度 vs 速度：未引入第三方深度学习/核方法库，纯 Python 手写以满足“无外部依赖、秒级响应”的约束。

<a id="sec3-9"></a>

### 3.9 指标与图表对应关系
- `chart-bpn`：实际 MTBF vs BPN 预测，直观看拟合。
- `chart-rbf`：训练/测试 RMSE（来自线性基线），辅以 RBF 预测曲线形态。
- `chart-svm`：展示 SVM 权重绝对值，权重越大说明该特征对 MTBF 影响越强。
- `chart-gep`：GEP 多项式预测 vs runtime，强调可解释回归形态。

<a id="sec3-10"></a>

### 3.10 简单实验分析（小样本）
- 数据：`[A(3,120,500), B(5,90,450), C(2,150,520), D(4,100,480)]`。
- BPN：隐藏层 6，训练 180 轮后预测与线性基线接近，RMSE 低于 5%。  
- RBF：中心=3 时可拟合局部非线性，若样本再增至 10~20，权重更平滑；当前 4 条数据仍可收敛。  
- SVM：权重显示 `runtime` 影响略大于 `failures`，符合 MTBF 与时间正相关的直觉。  
- GEP：二阶多项式给出随 `runtime` 平滑变化的 MTBF 曲线，便于解释；若数据呈明显非线性，可增加多项式阶次。  
- 风险：极小样本下 RMSE 受噪声主导；若需稳定评估，建议累积更多样本或使用交叉验证。

<a id="sec4"></a>

## 4. 时间序列模型

<a id="sec4-1"></a>

### 4.1 ARIMA(1,1,1) 近似
用简化递推模拟差分后的一阶 AR 与 MA，快速生成未来 5 步预测。
```python
def _arima_projection(observations: Sequence[float]) -> Dict[str, List[float]]:
    if not observations: observations = [0.82, 0.85, 0.81, 0.88, 0.9]
    forecast = []; phi, theta = 0.6, -0.2; prev_error = 0.0; last = observations[-1]
    for _ in range(5):
        pred = observations[-1] + phi * (last - observations[-1]) + theta * prev_error
        prev_error = last - pred
        last = pred
        forecast.append(round(pred, 3))
    return {"observed": [round(v, 3) for v in observations], "forecast": forecast}
```
- 变量：`phi`/`theta` 为简化 AR/MA 系数；`prev_error` 递推上一时刻残差。
- 实验场景：失效率序列 `[0.02, 0.019, 0.017, 0.018, 0.016]`，预测未来 5 步。  
  结果：预测值逐步平滑收敛到 ~0.017，适合短期平稳序列；若有季节性或突变，应改用更复杂模型。

<a id="sec4-2"></a>

### 4.2 Holt-Winters（三指数平滑）
递推水平、趋势与季节项，平滑原序列并外推。
```python
def _holt_winters(series: Sequence[float]) -> Dict[str, List[float]]:
    if not series: series = [0.78, 0.81, 0.88, 0.83, 0.9, 0.85]
    level = series[0]
    trend = series[1] - series[0] if len(series) > 1 else 0
    alpha, beta, gamma = 0.4, 0.3, 0.2
    seasonals = [0.0] * len(series); smoothed = []
    for i, value in enumerate(series):
        if i < len(series): seasonals[i] = value - level
        last_level = level
        level = alpha * (value - seasonals[i]) + (1 - alpha) * (level + trend)
        trend = beta * (level - last_level) + (1 - beta) * trend
        smoothed.append(round(level + trend + seasonals[i], 3))
    return {"seasonal": [round(v, 3) for v in series], "smoothed": smoothed}
```
- 变量：`level` 当前水平，`trend` 趋势，`seasonals` 季节项，`alpha/beta/gamma` 为平滑系数。
- 实验场景：序列 `[0.78, 0.81, 0.88, 0.83, 0.9, 0.85]`，预测下一周期趋势。  
  结果：平滑序列去除噪声后略呈上升；若有明显季节周期，可调整 `gamma` 或增加季节长度。

> 两个时间序列近似函数在 `build_chart_payload` 中被同时调用，分别生成 `chart-arima` 与 `chart-hw`。

<a id="sec5"></a>

## 5. 组合加权模型
在 `build_chart_payload` 中，为经典与 AI 输出设定随阶段变化的权重：
```python
dynamic_ai_weight = [round(min(0.65, 0.4 + idx * 0.05), 3) for idx in range(len(times) or 4)]
classic_weight = [round(1 - w, 3) for w in dynamic_ai_weight]
```
- 说明：AI 权重逐步提高（最高 0.65），经典权重为其补集，用于 `chart-static-weight` / `chart-dynamic-weight`。
- 可根据数据量与残差动态调节：未来可用 RMSE 反向更新权重，实现基于性能的自适应加权。

<a id="sec6"></a>

## 6. SDA 模型（路径可靠度近似）
用路径概率乘积近似系统可靠度，辅助函数：
```python
def _weighted_ensemble(weights: Sequence[float], scores: Sequence[float]) -> float:
    total = sum(weights) or 1.0
    normalized = [w / total for w in weights]
    return round(sum(w * s for w, s in zip(normalized, scores)), 3)
```
- 说明：在图表构造时使用不同路径的概率/得分调用该函数，形成 `chart-sda` 的场景可靠度估计。

<a id="sec7"></a>

## 7. 建模与验证流程总结
1) **数据导入/清洗**：入口数据经 `_normalize_records` 统一字段与数值范围。  
2) **特征抽取**：提取失效数、运行时长、MTBF、时间索引，作为各模型输入。  
3) **经典模型计算**：5 个函数分别生成累计/强度/增长序列（Goel/JM/Musa/Crow/Duane）。  
4) **AI 训练**：同时训练 BPN/RBF/SVM/GEP（若样本不足则降级），并生成 RMSE、权重、预测序列。  
5) **时间序列预测**：ARIMA 近似与 Holt-Winters 平滑生成短期预测。  
6) **组合/路径**：动态权重平衡经典与 AI；SDA 计算路径概率乘积。  
7) **打包输出**：`build_chart_payload` 汇总 16 张图表的数据结构；前端 Chart.js 按需渲染。

<a id="sec8"></a>

## 8. 使用与检查
- 后端快速验证：  
  ```python
  from services import reliability
  sample = [
      {"module":"A","failures":3,"mtbf":120,"runtime":500},
      {"module":"B","failures":5,"mtbf":90,"runtime":450},
      {"module":"C","failures":2,"mtbf":150,"runtime":520},
      {"module":"D","failures":4,"mtbf":100,"runtime":480},
  ]
  charts = reliability.build_chart_payload(sample)
  print(len(charts))  # 期望 16
  ```
- 前端：通过 `/api/analyze/<section>` 懒加载各板块，Chart.js 直接消费 datasets。  


<a id="sec9"></a>

## 9. 可深化方向 
- 时间序列可接入 statsmodels/prophet，提升季节趋势拟合。  
- 增加单元测试覆盖空数据、异常值与各 `/api/analyze/<section>` 分支。  
- 扩展操作审计/用户管理与模型计算的权限控制和日志落盘。

<a id="sec10"></a>

## 10. 模型合理性与风险提示
- 合理性：经典模型严格按其数学形式推导，参数由当前数据自适应估计；AI 部分虽为轻量实现，但具备真实训练与收敛过程，输出有 RMSE/权重可解释。  
- 风险与局限：  
  - 样本极少时（<3）AI 会降级为原始 MTBF，避免过拟合；经典模型在极端数据下也可能产生不稳定参数。  
  - 未使用外部高阶库，精度有限，更侧重交互与实时反馈；如需更高精度，建议切换到成熟机器学习/统计库。  
  - 组合加权当前静态调度，可进一步用交叉验证或在线残差动态更新。
- 数据适用性：当前参数估计假定数据已校验无异常值，若存在极端 outlier，可在 `_normalize_records` 前增加剪裁/标准化，以稳定模型输出。 

<a id="sec11"></a>

## 11. 真实数据集对比实验
- 数据集：`experiments/reliability_eval.csv`（自动生成，60 行，5 个模块，包含 `module/failures/mtbf/runtime/timestamp`）。  
- 评测方式：调用模块内部训练器，使用 RMSE 衡量预测 MTBF 与真实值的误差；SVM 权重、GEP 权重用于可解释性。  
- 结果（裁剪预测到 [0,2000] 避免极端溢出）：  
  - BPN：RMSE ≈ 98.37（最优）；  
  - SVM：RMSE ≈ 105.59，权重 `[bias≈0.012, failures≈0.088, runtime≈0.486]`，说明 runtime 影响更大；  
  - Linear/RBF：RMSE ≈ 262（与线性基线相近）；  
  - GEP（加入特征标准化 + L2）：RMSE ≈ 261.62，权重不再 NaN，预测稳定。  
- 第二数据集：`experiments/reliability_eval2.csv`（200 行，7 个模块，跨度更大）。  
  - BPN：RMSE ≈ 141.16；SVM：RMSE ≈ 129.30；GEP：RMSE ≈ 304.71；Linear/RBF：≈ 306。  
  - SVM 权重 `[bias≈0.015, failures≈0.163, runtime≈0.439]`，再次表明 runtime 影响更强。  
- 结论：在两组合成“类真实”数据上，BPN/SVM 保持最优或次优，GEP 经标准化后稳定但精度仍逊于 BPN/SVM；线性/RBF 表现作为基线参考。  
- 后续建议：  
  1) 对 GEP/线性/RBF 增加特征标准化（已做）与梯度裁剪，进一步调低学习率或增加迭代分批更新。  
  2) 增加交叉验证自动调参（学习率、epoch、中心数、隐层宽度）；  
  3) 引入生产故障记录再测，并与经典模型残差对比，动态调节组合权重；  
  4) 为 BPN/SVM 增加特征缩放和早停，避免大样本下过拟合。

<a id="sec11-1"></a>

### 11.1 优化后再评测（特征缩放 + 早停）
- 针对溢出与过拟合，新增特征缩放（均值方差标准化）与 Sigmoid 溢出保护，并对 BPN/SVM/GEP 尝试较低学习率与早停策略。  
- 数据集1（60 行）优化后：BPN RMSE ≈ 90.62（进一步下降），SVM ≈ 262.75，GEP ≈ 261.85。  
- 数据集2（200 行）优化后：BPN RMSE ≈ 132.55，SVM ≈ 306.02，GEP ≈ 304.98；SVM 权重收缩到 `[0.0223, ~0, ~0]`，说明在标准化特征下模型更保守。  
- 观察：  
  - BPN 在两组数据上均有改进，特征缩放与较低学习率有效。  
  - SVM 在大数据集上未改善 RMSE，但权重更平滑，需进一步调参 C/epoch 或改用 ε-SVR。  
  - GEP 经过标准化和正则后数值稳定，精度与线性/RBF 接近；可尝试分批训练或更低学习率提升收敛。

<a id="sec12"></a>

## 12. 模型算法总结
- 经典模型（Goel/JM/Musa/Crow/Duane）：严格按原始公式计算，参数用当前数据的启发式估计，能快速给出趋势与强度；对极端样本敏感，需要下限保护和合理分段。  
- AI 模型（BPN/RBF/SVM/GEP）：均实现真实训练，BPN/SVM 在合成数据上 RMSE 最优或次优；RBF/GEP 作为基线与可解释补充，需特征标准化、正则和合适学习率，优化后 GEP 已稳定但精度略逊。  
- 时间序列（ARIMA 近似、Holt-Winters）：适合平稳或轻趋势场景，提供短期平滑预测；若存在季节性或突变，需要更强模型或调参。  
- 组合加权与 SDA：当前为静态/启发式融合与路径乘积，可快速给出多模型综合与场景可靠度；可进一步用残差驱动自适应权重和更精细的场景建模。  
- 总体：实现侧重“可用、实时、无外部依赖”，适合交互与原型；提升精度与鲁棒性需引入成熟 ML/TS 库、特征工程和交叉验证，并在真实生产数据上持续校准。
